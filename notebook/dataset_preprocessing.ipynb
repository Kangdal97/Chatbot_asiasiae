{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "24536bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset # 데이터로더\n",
    "\n",
    "from kogpt2_transformers import get_kogpt2_tokenizer\n",
    "from kobert_transformers import get_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f658e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WellnessAutoRegressiveDataset(Dataset):  # 질문과 답변 쌍 데이터\n",
    "    def __init__(self,\n",
    "                   file_path = \"../data/wellness_dialog_for_autoregressive_seungwoo.txt\",\n",
    "                   n_ctx = 1024\n",
    "                   ):\n",
    "        self.file_path = file_path\n",
    "        self.data =[]     # index_of_words  가 들어감.\n",
    "        self.tokenizer = get_kogpt2_tokenizer()\n",
    "\n",
    "\n",
    "        bos_token_id = [self.tokenizer.bos_token_id]    # 출력하면 [0] 값.\n",
    "        eos_token_id = [self.tokenizer.eos_token_id]    # 출력하면 [1] 값.\n",
    "        pad_token_id = [self.tokenizer.pad_token_id]    # 출력하면 [3] 값.\n",
    "\n",
    "        file = open(self.file_path, 'r', encoding='utf-8')\n",
    "\n",
    "        while True:\n",
    "            line = file.readline()\n",
    "            if not line:\n",
    "                break\n",
    "            datas = line.split(\"    \")\n",
    "            #     print(datas)\n",
    "            #     ['제 감정이 이상해진 것 같아요. 남편만 보면 화가 치밀어 오르고 감정 조절이 안되요.', '감정이 조절이 안 될 때만큼 힘들 때는 없는 거 같아요.\\n']\n",
    "            #     ['제 감정이 이상해진 것 같아요. 남편만 보면 화가 치밀어 오르고 감정 조절이 안되요.', '저도 그 기분 이해해요. 많이 힘드시죠?\\n']\n",
    "\n",
    "            index_of_words = bos_token_id +self.tokenizer.encode(datas[0]) + eos_token_id + bos_token_id + self.tokenizer.encode(datas[1][:-1])+ eos_token_id\n",
    "            pad_token_len = n_ctx - len(index_of_words)\n",
    "\n",
    "            index_of_words += pad_token_id * pad_token_len\n",
    "\n",
    "            self.data.append(index_of_words)\n",
    "        \n",
    "        print('tokenizer.encode(datas[0]):', tokenizer.encode(datas[0]))\n",
    "        tokenizer.encode(datas[0]): [47529, 47674]\n",
    "        tokenizer.encode(datas[0]): [47481, 30790, 389, 3529, 120, 13996, 47440, 3502, 47494, 1767, 11388, 45585, 47487, 10992, 7212, 5680, 47438, 199, 47562, 47580, 47440] \n",
    "        print('tokenizer.encode(datas[1][:-1]):', tokenizer.encode(datas[1][:-1]))\n",
    "        tokenizer.encode(datas[1][:-1]): [49251, 48326]\n",
    "        tokenizer.encode(datas[1][:-1]): [47640, 1968, 5680, 47438, 199, 737, 215, 2747, 4171, 3161, 779, 241, 13996, 47440]\n",
    "        print(index_of_words)\n",
    "        [0, 47529, 47674, 1, 0, 49251, 48326, 1]\n",
    "        [0, 47481, 30790, 389, 3529, 120, 13996, 47440, 3502, 47494, 1767, 11388, 45585, 47487, 10992, 7212, 5680, 47438, 199, 47562, 47580, 47440, 1, 0, 47640, 1968, 5680, 47438, 199, 737, 215, 2747, 4171, 3161, 779, 241, 13996, 47440, 1]\n",
    "\n",
    "        file.close()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)   ### data = index_of_words\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        item = self.data[index]\n",
    "        return item\n",
    "    \n",
    "########################################################################################\n",
    "class WellnessTextClassificationDataset(Dataset):\n",
    "    \n",
    "    def __init__(self,\n",
    "               file_path = \"../data/wellness_dialog_for_text_classification_seungwoo.txt\",\n",
    "               num_label = 359,\n",
    "               device = 'cpu',\n",
    "               max_seq_len = 512,  # KoBERT max_length\n",
    "               tokenizer = None\n",
    "               ):\n",
    "        self.file_path = file_path\n",
    "        self.device = device\n",
    "        self.data =[]\n",
    "        self.tokenizer = tokenizer if tokenizer is not None else get_tokenizer()\n",
    "\n",
    "\n",
    "        file = open(self.file_path, 'r', encoding='utf-8')\n",
    "\n",
    "        while True:\n",
    "            line = file.readline()\n",
    "            if not line:\n",
    "                break\n",
    "            datas = line.split(\"    \")\n",
    "            #     print(datas)\n",
    "            #     ['제 감정이 이상해진 것 같아요. 남편만 보면 화가 치밀어 오르고 감정 조절이 안되요.', '0\\n']\n",
    "            #     ['더 이상 내 감정을 내가 컨트롤 못 하겠어.', '0\\n']\n",
    "            index_of_words = self.tokenizer.encode(datas[0])\n",
    "            token_type_ids = [0] * len(index_of_words)\n",
    "            attention_mask = [1] * len(index_of_words)\n",
    "\n",
    "            # Padding Length\n",
    "            padding_length = max_seq_len - len(index_of_words)\n",
    "\n",
    "            # Zero Padding\n",
    "            index_of_words += [0] * padding_length\n",
    "            token_type_ids += [0] * padding_length\n",
    "            attention_mask += [0] * padding_length\n",
    "\n",
    "            # Label\n",
    "            label = int(datas[1][:-1])\n",
    "\n",
    "            data = {\n",
    "                  'input_ids': torch.tensor(index_of_words).to(self.device),\n",
    "                  'token_type_ids': torch.tensor(token_type_ids).to(self.device),\n",
    "                  'attention_mask': torch.tensor(attention_mask).to(self.device),\n",
    "                  'labels': torch.tensor(label).to(self.device)\n",
    "                 }\n",
    "\n",
    "            self.data.append(data)\n",
    "        \n",
    "        len(data_dict[i]: 'input_ids','token_type_ids','attention_mask')의 길이는 모두 512\n",
    "        data_dict['labels']은 그냥 레이블 값\n",
    "        len(data) : 5231    \n",
    "\n",
    "        {'input_ids': tensor([   2, 1370, 5859, 3647, 6037, 5561,  517, 6744, 7086, 5850, 1434, 1917,\n",
    "                 5812, 3135, 5876,   54,    3,    0,    0, ... 0, 0])\n",
    "        'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "                 0, 0, 0, 0, 0, 0, 0, 0, 0, ..., 0, 0, 0, 0, 0])\n",
    "        'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
    "                 0, 0, 0, 0, 0, 0, 0, 0, ..., 0, 0])\n",
    "        'labels': tensor(0)} \n",
    "          \n",
    "        file.close()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getitem__(self,index):\n",
    "        item = self.data[index]\n",
    "        return item\n",
    "    \n",
    "    \n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    dataset = WellnessAutoRegressiveDataset()\n",
    "    dataset2 = WellnessTextClassificationDataset()\n",
    "    print(dataset)\n",
    "    print(dataset2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c84d8880",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset # 데이터로더\n",
    "from kogpt2_transformers import get_kogpt2_tokenizer\n",
    "from kobert_transformers import get_tokenizer\n",
    "\n",
    "\n",
    "file_path = \"../data/wellness_dialog_for_text_classification_seungwoo.txt\"\n",
    "num_label = 359\n",
    "device = 'cpu'\n",
    "max_seq_len = 512  # KoBERT max_length\n",
    "tokenizer = None\n",
    "###############################\n",
    "file_path = file_path\n",
    "device = device\n",
    "data =[]\n",
    "tokenizer = tokenizer if tokenizer is not None else get_tokenizer()\n",
    "\n",
    "file = open(file_path, 'r', encoding='utf-8')\n",
    "\n",
    "while True:\n",
    "# for i in range(10) :\n",
    "    line = file.readline()\n",
    "    if not line:\n",
    "        break\n",
    "    datas = line.split(\"    \")\n",
    "#     print(datas)\n",
    "#     ['제 감정이 이상해진 것 같아요. 남편만 보면 화가 치밀어 오르고 감정 조절이 안되요.', '0\\n']\n",
    "#     ['더 이상 내 감정을 내가 컨트롤 못 하겠어.', '0\\n']\n",
    "    \n",
    "    index_of_words = tokenizer.encode(datas[0])\n",
    "    token_type_ids = [0] * len(index_of_words)\n",
    "    attention_mask = [1] * len(index_of_words)\n",
    "\n",
    "    # Padding Length\n",
    "    padding_length = max_seq_len - len(index_of_words)\n",
    "\n",
    "    # Zero Padding\n",
    "    index_of_words += [0] * padding_length\n",
    "    token_type_ids += [0] * padding_length\n",
    "    attention_mask += [0] * padding_length\n",
    "\n",
    "    # Label\n",
    "    label = int(datas[1][:-1])\n",
    "    \n",
    "    data_dict = {\n",
    "          'input_ids': torch.tensor(index_of_words).to(device),\n",
    "          'token_type_ids': torch.tensor(token_type_ids).to(device),\n",
    "          'attention_mask': torch.tensor(attention_mask).to(device),\n",
    "          'labels': torch.tensor(label).to(device)\n",
    "         }\n",
    "\n",
    "    data.append(data_dict)\n",
    "\n",
    "# len(data_dict[i]: 'input_ids','token_type_ids','attention_mask')의 길이는 모두 512\n",
    "# data_dict['labels']은 그냥 레이블 값\n",
    "# len(data) : 5231    \n",
    "    \n",
    "# {'input_ids': tensor([   2, 1370, 5859, 3647, 6037, 5561,  517, 6744, 7086, 5850, 1434, 1917,\n",
    "#          5812, 3135, 5876,   54,    3,    0,    0, ... 0, 0])\n",
    "# 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "#          0, 0, 0, 0, 0, 0, 0, 0, 0, ..., 0, 0, 0, 0, 0])\n",
    "# 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
    "#          0, 0, 0, 0, 0, 0, 0, 0, ..., 0, 0])\n",
    "# 'labels': tensor(0)} \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "file.close()\n",
    "        \n",
    "#     def __len__(self):\n",
    "#         return len(self.data)\n",
    "#     def __getitem__(self,index):\n",
    "#         item = self.data[index]\n",
    "#         return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5b5cc9ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5231"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
